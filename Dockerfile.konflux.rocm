## Global Args ##################################################################
ARG BASE_UBI_IMAGE_TAG=9.5-1742914212
ARG PYTHON_VERSION=3.12
ARG VLLM_VERSION="v0.10.0.2"
# Default ROCm ARCHes to build vLLM for.
ARG PYTORCH_ROCM_ARCH="gfx908;gfx90a;gfx942;gfx1100"
ARG MAX_JOBS=12
ARG VLLM_TGIS_ADAPTER_VERSION=0.8.0

FROM registry.access.redhat.com/ubi9/ubi-minimal:${BASE_UBI_IMAGE_TAG} AS base

ARG PYTHON_VERSION

ENV VIRTUAL_ENV=/opt/vllm
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

RUN --mount=type=cache,target=/root/.cache/pip \
 microdnf -y update && \
 microdnf install -y --setopt=install_weak_deps=0 --nodocs \
    python${PYTHON_VERSION}-devel \
    python${PYTHON_VERSION}-pip \
    python${PYTHON_VERSION}-wheel && \
    python${PYTHON_VERSION} -m venv $VIRTUAL_ENV && \
    pip install -U pip wheel setuptools uv && \
 microdnf clean all


FROM base AS rocm_base
ARG ROCM_VERSION=6.3.4
ARG PYTHON_VERSION
ARG BASE_UBI_IMAGE_TAG

RUN printf "[amdgpu]\n\
name=amdgpu\n\
baseurl=https://repo.radeon.com/amdgpu/${ROCM_VERSION}/rhel/${BASE_UBI_IMAGE_TAG/-*/}/main/x86_64/\n\
enabled=1\n\
priority=50\n\
gpgcheck=1\n\
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key\n\
[ROCm-${ROCM_VERSION}]\n\
name=ROCm${ROCM_VERSION}\n\
baseurl=https://repo.radeon.com/rocm/rhel9/${ROCM_VERSION}/main\n\
enabled=1\n\
priority=50\n\
gpgcheck=1\n\
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key" > /etc/yum.repos.d/amdgpu.repo


RUN --mount=type=cache,target=/root/.cache/uv \
    export version="$(awk -F. '{print $1"."$2}' <<< $ROCM_VERSION)" && \
    uv pip install --pre \
        --index-url "https://download.pytorch.org/whl/rocm${version}" \
        torch==2.7.0+rocm${version}\
        torchvision==0.22.0+rocm${version} && \
    # Install libdrm-amdgpu to avoid errors when retrieving device information (amdgpu.ids: No such file or directory)
    microdnf install -y --nodocs libdrm-amdgpu && \
    microdnf clean all


ENV LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/numpy.libs:$LD_LIBRARY_PATH"
ENV LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/pillow.libs:$LD_LIBRARY_PATH"
ENV LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/triton/backends/amd/lib:$LD_LIBRARY_PATH"
ENV LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/torch/lib:$LD_LIBRARY_PATH"

RUN echo $LD_LIBRARY_PATH | tr : \\n >> /etc/ld.so.conf.d/torch-venv.conf && \
    ldconfig

FROM rocm_base as rocm_devel

ENV CCACHE_DIR=/root/.cache/ccache

RUN rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm && \
    rpm -ql epel-release && \
    microdnf -y update && \
    microdnf --nodocs -y install \
        ccache \
        git \
        # packages required to build vllm
        amd-smi-lib \
        hipblas-devel \
        hipblaslt-devel \
        hipcc \
        hipcub-devel \
        hipfft-devel \
        hiprand-devel \
        hipsolver-devel \
        hipsparse-devel \
        hsa-rocr-devel \
        miopen-hip-devel \
        rccl-devel \
        rocblas-devel \
        rocm-device-libs \
        rocprim-devel \
        rocrand-devel \
        rocthrust-devel \
        # end packages required to build vllm
        wget \
        which && \
    microdnf clean all

WORKDIR /workspace

ENV LLVM_SYMBOLIZER_PATH=/opt/rocm/llvm/bin/llvm-symbolizer
ENV PATH=$PATH:/opt/rocm/bin
ENV CPLUS_INCLUDE_PATH=$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/torch/include:/opt/rocm/include


FROM rocm_devel AS build_amdsmi

# Build AMD SMI wheel
RUN cd /opt/rocm/share/amd_smi && \
    python3 -m pip wheel . --wheel-dir=/install

##################################################################################################

FROM rocm_devel AS build_flashattention

ARG FA_GFX_ARCHS="gfx90a;gfx942"

# the FA_BRANCH commit belongs to the ROCm/flash-attention fork, `main_perf` branch
ARG FA_BRANCH="1a7f4dfa"
ARG MAX_JOBS
ENV MAX_JOBS=${MAX_JOBS}

RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=cache,target=/workspace/build \
    mkdir -p /libs && \
    cd /libs && \
    git clone https://github.com/ROCm/flash-attention.git && \
    cd flash-attention && \
    git checkout ${FA_BRANCH} && \
    git submodule update --init && \
    uv pip install "cmake<4" ninja packaging && \
    env \
        GPU_ARCHS="${FA_GFX_ARCHS}" \
        python3 setup.py bdist_wheel --dist-dir=/install

##################################################################################################

FROM rocm_devel AS build_vllm
ARG PYTORCH_ROCM_ARCH
ARG PYTHON_VERSION
ARG MAX_JOBS
ARG VLLM_VERSION
ENV MAX_JOBS=${MAX_JOBS}
ENV PYTORCH_ROCM_ARCH=${PYTORCH_ROCM_ARCH}

COPY . .

ENV VLLM_TARGET_DEVICE="rocm"
ENV MAX_JOBS=${MAX_JOBS}
# Make sure punica kernels are built (for LoRA)
ENV VLLM_INSTALL_PUNICA_KERNELS=1

RUN --mount=type=cache,target=/root/.cache/ccache \
    --mount=type=cache,target=/root/.cache/pip \
    --mount=type=cache,target=/root/.cache/uv \
    uv pip install -v -U \
        ninja setuptools-scm>=8 "cmake>=3.26,<4" packaging && \
    env CFLAGS="-march=haswell" \
        CXXFLAGS="$CFLAGS $CXXFLAGS" \
        CMAKE_BUILD_TYPE=Release \
        SETUPTOOLS_SCM_PRETEND_VERSION="$VLLM_VERSION" \
    python3 setup.py bdist_wheel --dist-dir=dist

##################################################################################################

FROM rocm_base AS vllm-openai
ARG MAX_JOBS
ARG FLASH_ATTENTION_WHEEL_STRATEGY
ARG VLLM_WHEEL_STRATEGY
ARG ROCM_VERSION

WORKDIR /workspace

ENV VIRTUAL_ENV=/opt/vllm
ENV PATH=$VIRTUAL_ENV/bin:$PATH

# Required for triton
RUN microdnf install -y --setopt=install_weak_deps=0 --nodocs gcc rsync && \
    microdnf clean all

RUN --mount=type=bind,from=build_amdsmi,src=/install,target=/install/amdsmi/ \
    --mount=type=bind,from=build_flashattention,src=/install,target=/install/flashattention \
    --mount=type=bind,from=build_vllm,src=/workspace/dist,target=/install/vllm/ \
    --mount=type=cache,target=/root/.cache/pip \
    --mount=type=cache,target=/root/.cache/uv \
    export version="$(awk -F. '{print $1"."$2}' <<< $ROCM_VERSION)" && \
    uv pip install \
        --index-strategy=unsafe-best-match \
        --extra-index-url "https://download.pytorch.org/whl/rocm${version}" \
        /install/amdsmi/*.whl \
        /install/flashattention/*.whl \
        "$(echo /install/vllm/*.whl)[audio,video,tensorizer]" \
        blobfile

# [TEMP] Install triton packages (need to be pinned for v0.10.0) in sequence to avoid conflicts
RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=cache,target=/root/.cache/uv \
    export version="$(awk -F. '{print $1"."$2}' <<< $ROCM_VERSION)" && \
    uv pip install --force-reinstall \
        --extra-index-url "https://download.pytorch.org/whl/rocm${version}" \
        triton==3.3.0 && \
    uv pip install --force-reinstall \
        --find-links "https://download.pytorch.org/whl/pytorch-triton-rocm/" \
        pytorch-triton-rocm==3.3.0

ENV HF_HUB_OFFLINE=1 \
    HOME=/home/vllm \
    # Allow requested max length to exceed what is extracted from the
    # config.json
    # see: https://github.com/vllm-project/vllm/pull/7080
    VLLM_USAGE_SOURCE=production-docker-image \
    VLLM_WORKER_MULTIPROC_METHOD=spawn \
    VLLM_NO_USAGE_STATS=1 \
    # Silences the HF Tokenizers warning
    TOKENIZERS_PARALLELISM=false  \
    RAY_EXPERIMENTAL_NOSET_ROCR_VISIBLE_DEVICES=1 \
    VLLM_USE_TRITON_FLASH_ATTN=0 \
    HIP_FORCE_DEV_KERNARG=1 \
    OUTLINES_CACHE_DIR=/tmp/outlines \
    NUMBA_CACHE_DIR=/tmp/numba \
    TRITON_CACHE_DIR=/tmp/triton

# setup non-root user for OpenShift
RUN umask 002 && \
    useradd --uid 2000 --gid 0 vllm && \
    mkdir -p /licenses /home/vllm && \
    chmod g+rwx /home/vllm

COPY LICENSE /licenses/vllm.md
COPY examples/*.jinja /app/data/template/
COPY examples/*.jinja /opt/app-root/template/
RUN chown -R vllm /opt/app-root/template && chmod -R g+r /opt/app-root/template
# RUN mkdir -p /opt/app-root && \
#    ln -s /app/data/template /opt/app-root/template

USER 2000
WORKDIR /home/vllm

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]


FROM vllm-openai as vllm-grpc-adapter

USER root

ARG VLLM_TGIS_ADAPTER_VERSION
RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,from=build_vllm,src=/workspace/dist,target=/install/vllm/ \
    HOME=/root uv pip install \
        "$(echo /install/vllm/*.whl)[audio,video,tensorizer]" \
        vllm-tgis-adapter==${VLLM_TGIS_ADAPTER_VERSION}

ENV GRPC_PORT=8033 \
    PORT=8000 \
    # As an optimization, vLLM disables logprobs when using spec decoding by
    # default, but this would be unexpected to users of a hosted model that
    # happens to have spec decoding
    # see: https://github.com/vllm-project/vllm/pull/6485
    DISABLE_LOGPROBS_DURING_SPEC_DECODING=false

USER 2000
ENTRYPOINT ["python3", "-m", "vllm_tgis_adapter", "--uvicorn-log-level=warning"]

LABEL name="rhoai/odh-vllm-rocm-rhel9" \
      com.redhat.component="odh-vllm-rocm-rhel9" \
      io.k8s.display-name="odh-vllm-rocm-rhel9" \
      io.k8s.description="vLLM build leveraging AMD ROCm for GPU-based inference on AMD hardware." \
      description="vLLM build leveraging AMD ROCm for GPU-based inference on AMD hardware." \
      summary="vLLM build leveraging AMD ROCm for GPU-based inference on AMD hardware." \
      com.redhat.license_terms="https://www.redhat.com/licenses/Red_Hat_Standard_EULA_20191108.pdf"